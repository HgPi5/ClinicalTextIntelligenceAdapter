{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: \n",
    "* Model: \n",
    "* Evaluation approach: \n",
    "* Fine-tuning dataset: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a medical diagnosis dataset with 10 different diseases as labels\n",
    "# ds = \"Deysi/spam-detection-dataset\"\n",
    "ds = \"ninaa510/diagnosis-text\"\n",
    "orig_dataset = load_dataset(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ca5ce7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting string labels to numerical. This can be mapped back during inference at the end\n",
    "label_to_int = {'Allergic sinusitis' : 0\n",
    "            , 'Anaphylaxis' : 1\n",
    "            , 'Chagas' : 2\n",
    "            , 'HIV (initial infection)' : 3\n",
    "            , 'Influenza' : 4\n",
    "            , 'Localized edema' : 5\n",
    "            , 'SLE' : 6\n",
    "            , 'Sarcoidosis' : 7\n",
    "            , 'Tuberculosis' : 8\n",
    "            , 'Whooping cough' : 9}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb2b3f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text labels: ['Allergic sinusitis', 'Anaphylaxis', 'Chagas', 'HIV (initial infection)', 'Influenza', 'Localized edema', 'SLE', 'Sarcoidosis', 'Tuberculosis', 'Whooping cough']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, ClassLabel, Features, Value\n",
    "\n",
    "# Get the unique text labels to define the ClassLabel feature\n",
    "unique_labels = sorted(list(set(orig_dataset[\"train\"][\"label\"])))\n",
    "print(f\"Original text labels: {unique_labels}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb20ecdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': ClassLabel(names=['Allergic sinusitis', 'Anaphylaxis', 'Chagas', 'HIV (initial infection)', 'Influenza', 'Localized edema', 'SLE', 'Sarcoidosis', 'Tuberculosis', 'Whooping cough']),\n",
       " 'sentence1': Value('string')}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the new features, including the ClassLabel for the 'label' column for integer mapping\n",
    "\n",
    "new_features = orig_dataset['train'].features.copy()\n",
    "new_features['label'] = ClassLabel(names=unique_labels)\n",
    "\n",
    "new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "778fcbf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 9,\n",
       " 'sentence1': 'As a 53-year-old female, my main symptoms include vomiting and coughing, along with diarrhea, pain in the upper abdomen described as a knife stroke, skin lesions or rashes on my right ankle, nausea, shortness of breath, swelling in my nose area, and an allergic reaction. The rash is pink in color.'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cast the dataset columns to the new feature types\n",
    "# This automatically handles the conversion from string to integer based on the ClassLabel.\n",
    "dataset = orig_dataset.cast(new_features)\n",
    "\n",
    "dataset['train'][11979]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "494d6a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features of the new dataset:\n",
      "{'label': ClassLabel(names=['Allergic sinusitis', 'Anaphylaxis', 'Chagas', 'HIV (initial infection)', 'Influenza', 'Localized edema', 'SLE', 'Sarcoidosis', 'Tuberculosis', 'Whooping cough']), 'sentence1': Value('string')}\n",
      "\n",
      "First 5 examples with integer labels:\n",
      "{'label': [0, 0, 0, 0, 0], 'sentence1': ['I am a 12-year-old male with an itchy nose, sharp upper abdominal pain, pink skin lesions or rashes on my neck, swelling in my nose, had an allergic reaction, and experiencing high-pitched breathing sounds, lightheadedness, and wheezing on exhale.', 'I am suffering from eye itching with pain, skin rashes, allergic reactions, shortness of breath, swelling, loss of consciousness, high-pitched breathing, and wheezing on exhale.', 'I am a 20-year-old female suffering from an itchy nose, heavy pain in my left top of the foot, and swelling in my right sole.', 'I am a 68-year-old male with eye itching as the main symptom, accompanied by shortness of breath, cough, and coughing up blood.', 'I am a 24-year-old female with runny nose and swollen lymph nodes, pain characterized as burning in my left shoulder, and vaginal discharge.']}\n",
      "\n",
      "Label mapping (integer to string): Tuberculosis\n"
     ]
    }
   ],
   "source": [
    "# The 'label' column will contain integers instead of strings.\n",
    "# You can see the mapping by inspecting the features of the new dataset.\n",
    "\n",
    "print(\"Features of the new dataset:\")\n",
    "print(dataset[\"train\"].features)\n",
    "\n",
    "print(\"\\nFirst 5 examples with integer labels:\")\n",
    "print(dataset[\"train\"][:5])\n",
    "\n",
    "# You can still access the original text labels by using int2str()\n",
    "label_feature = dataset[\"train\"].features[\"label\"]\n",
    "print(f\"\\nLabel mapping (integer to string): {label_feature.int2str(8)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "019b9f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(dataset[\"train\"]['label'])\n",
    "# type(dataset[\"train\"])\n",
    "# dataset['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc5b5b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    1331\n",
       "1    1331\n",
       "2    1331\n",
       "3    1331\n",
       "4    1331\n",
       "5    1331\n",
       "6    1331\n",
       "7    1331\n",
       "8    1331\n",
       "9    1331\n",
       "Name: sentence1, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of samples of each label in the train dataset\n",
    "import datasets\n",
    "import pandas as pd\n",
    "\n",
    "pandas_df = dataset[\"train\"].to_pandas()\n",
    "\n",
    "# Perform groupby operation using pandas\n",
    "grouped_df = pandas_df.groupby(\"label\")[\"sentence1\"].count()\n",
    "\n",
    "grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"]\n",
    "splits = [\"train\", \"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48f3c478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking only 5000 from train and 1000 from test splits\n",
    "dataset[\"train\"] = dataset[\"train\"].shuffle(seed = 41).select(range(5000))\n",
    "dataset[\"test\"] = dataset[\"test\"].shuffle(seed = 41).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96f8f134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    504\n",
       "1    524\n",
       "2    495\n",
       "3    484\n",
       "4    519\n",
       "5    485\n",
       "6    496\n",
       "7    494\n",
       "8    500\n",
       "9    499\n",
       "Name: sentence1, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking label counts after subsetting the dataset. To see if they are evenly spread\n",
    "pandas_df = dataset[\"train\"].to_pandas()\n",
    "\n",
    "# Perform groupby operation using pandas\n",
    "grouped_df = pandas_df.groupby(\"label\")[\"sentence1\"].count()\n",
    "\n",
    "grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62b50105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69de342b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'sentence1'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"]\n",
    "dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3476893e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 4,\n",
       " 'sentence1': 'My main symptom is pain accompanied by an itchy nose or throat and nasal congestion.'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]\n",
    "# max(dataset[\"train\"][\"label\"])\n",
    "# min(dataset[\"train\"][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3fd9b3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating 2 different models to check which one performs better\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer \n",
    "\n",
    "# model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "# model_name = \"distilbert-base-uncased\"\n",
    "# model_name = \"roberta-base\"\n",
    "model_name = \"medicalai/ClinicalBERT\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd41d4ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a7e290c11d84ac099540ba4229d5afc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'sentence1', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 5000\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from transformers import DistilBertTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize all examples\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence1\"], padding = \"max_length\", truncation = True, max_length = 512)\n",
    "\n",
    "tokenized_ds = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_ds[\"train\"]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c1562e3",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenized_ds[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aebcc355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenized_ds[\"test\"][0]['input_ids']\n",
    "len(tokenized_ds[\"train\"][10]['input_ids'])\n",
    "# tokenized_ds[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30b3241c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_labels = 2\n",
    "# id2label = {0: 'not_spam', 1: 'spam'}\n",
    "# label2id = {v: k for k,v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "894046c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eed096da7b24cdfa5c8a5fc4a45a28d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/542M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at medicalai/ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "527bc000c2584a56b16b2f3b7d9ff258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/542M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "# import tensorflow\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=10,\n",
    "#     from_tf = True\n",
    "#     id2label=id2label,\n",
    "#     label2id=label2id,\n",
    "    ignore_mismatched_sizes=True\n",
    "\n",
    ")\n",
    "\n",
    "# Freeze all the model parameters.\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4d4c908",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e65d01ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./results/bert_10-02_20:45:51\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime as dt\n",
    "output_dir=\"./results/bert_\" + dt.now().strftime(\"%m-%d_%H:%M:%S\")\n",
    "print(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95be9dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/29/qhr03k_j69n5gqmc_lrlm_lr0000gn/T/ipykernel_19067/3081607791.py:23: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n",
    "model_args=TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate = 2e-5, \n",
    "    per_device_train_batch_size = 10, \n",
    "    per_device_eval_batch_size = 10, \n",
    "    eval_strategy = \"epoch\", \n",
    "    save_strategy = \"epoch\", \n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    remove_unused_columns=True,\n",
    "\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=model_args, \n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b47abf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3699d49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harithagollakota/Desktop/Py_Projects_Mini/deeplearn_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.27618145942688,\n",
       " 'eval_model_preparation_time': 0.0009,\n",
       " 'eval_accuracy': 0.126,\n",
       " 'eval_runtime': 19.406,\n",
       " 'eval_samples_per_second': 51.531,\n",
       " 'eval_steps_per_second': 5.153}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e8a663",
   "metadata": {},
   "source": [
    "###  ⚠️ IMPORTANT ⚠️\n",
    "\n",
    "Due to workspace storage constraints, you should not store the model weights in the same directory but rather use `/tmp` to avoid workspace crashes which are irrecoverable.\n",
    "Ensure you save it in /tmp always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at medicalai/ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraModel, LoraConfig\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "#     target_modules=[\"q_lin\", \"k_lin\", \"v_lin\", \"out_lin\", \"lin1\", \"lin2\"],\n",
    "    target_modules = [\"q_lin\", \"k_lin\", \"v_lin\", \"out_lin\"],\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=10,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model\n",
    "lora_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c09ce03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 893,194 || all params: 136,225,556 || trainable%: 0.6557\n"
     ]
    }
   ],
   "source": [
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "051490c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./results/bert_lora_10-02_20:49:18\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime as dt\n",
    "output_dir=\"./results/bert_lora_\" + dt.now().strftime(\"%m-%d_%H:%M:%S\")\n",
    "print(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "866ab28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/29/qhr03k_j69n5gqmc_lrlm_lr0000gn/T/ipykernel_19067/2155753919.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  lora_trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 28:51, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.306700</td>\n",
       "      <td>2.294745</td>\n",
       "      <td>0.147000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.206500</td>\n",
       "      <td>1.873657</td>\n",
       "      <td>0.398000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.675500</td>\n",
       "      <td>1.522989</td>\n",
       "      <td>0.463000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.484600</td>\n",
       "      <td>1.416093</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.423600</td>\n",
       "      <td>1.387991</td>\n",
       "      <td>0.506000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harithagollakota/Desktop/Py_Projects_Mini/deeplearn_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/harithagollakota/Desktop/Py_Projects_Mini/deeplearn_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/harithagollakota/Desktop/Py_Projects_Mini/deeplearn_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/harithagollakota/Desktop/Py_Projects_Mini/deeplearn_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2500, training_loss=1.819380810546875, metrics={'train_runtime': 1733.1732, 'train_samples_per_second': 14.424, 'train_steps_per_second': 1.442, 'total_flos': 3380754739200000.0, 'train_loss': 1.819380810546875, 'epoch': 5.0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "# 5 or 10 epochs is fine\n",
    "import numpy as np\n",
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n",
    "lora_train_args=TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        learning_rate = 2e-5, \n",
    "        per_device_train_batch_size = 10, \n",
    "        per_device_eval_batch_size = 10, \n",
    "        eval_strategy = \"epoch\", \n",
    "        save_strategy = \"epoch\", \n",
    "        num_train_epochs=5,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True\n",
    "    )\n",
    "\n",
    "lora_trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args = lora_train_args, \n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "lora_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f9a32e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harithagollakota/Desktop/Py_Projects_Mini/deeplearn_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.387990951538086,\n",
       " 'eval_accuracy': 0.506,\n",
       " 'eval_runtime': 22.5722,\n",
       " 'eval_samples_per_second': 44.302,\n",
       " 'eval_steps_per_second': 4.43,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cbda58e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "lora_model.save_pretrained(\"lora_peft_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "184b07b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForSequenceClassification(\n",
      "  (base_model): LoraModel(\n",
      "    (model): DistilBertForSequenceClassification(\n",
      "      (distilbert): DistilBertModel(\n",
      "        (embeddings): Embeddings(\n",
      "          (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
      "          (position_embeddings): Embedding(512, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (transformer): Transformer(\n",
      "          (layer): ModuleList(\n",
      "            (0-5): 6 x TransformerBlock(\n",
      "              (attention): DistilBertSdpaAttention(\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (q_lin): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (k_lin): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (v_lin): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (out_lin): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "              (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (ffn): FFN(\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (activation): GELUActivation()\n",
      "              )\n",
      "              (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pre_classifier): ModulesToSaveWrapper(\n",
      "        (original_module): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (modules_to_save): ModuleDict(\n",
      "          (default): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (classifier): ModulesToSaveWrapper(\n",
      "        (original_module): Linear(in_features=768, out_features=10, bias=True)\n",
      "        (modules_to_save): ModuleDict(\n",
      "          (default): Linear(in_features=768, out_features=10, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79333f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at medicalai/ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from peft import AutoPeftModelForSequenceClassification\n",
    "lora_peft = AutoPeftModelForSequenceClassification.from_pretrained(\n",
    "    \"lora_peft_model\"\n",
    "    # model = \"/Users/harithagollakota/Downloads/LightweightFineTuning/results/bert_lora_10-02_20:49:18\"\n",
    "    , num_labels=10\n",
    "    , ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e407b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForSequenceClassification(\n",
      "  (base_model): LoraModel(\n",
      "    (model): DistilBertForSequenceClassification(\n",
      "      (distilbert): DistilBertModel(\n",
      "        (embeddings): Embeddings(\n",
      "          (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
      "          (position_embeddings): Embedding(512, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (transformer): Transformer(\n",
      "          (layer): ModuleList(\n",
      "            (0-5): 6 x TransformerBlock(\n",
      "              (attention): DistilBertSdpaAttention(\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (q_lin): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (k_lin): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (v_lin): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (out_lin): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "              (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (ffn): FFN(\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (activation): GELUActivation()\n",
      "              )\n",
      "              (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pre_classifier): ModulesToSaveWrapper(\n",
      "        (original_module): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (modules_to_save): ModuleDict(\n",
      "          (default): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (classifier): ModulesToSaveWrapper(\n",
      "        (original_module): Linear(in_features=768, out_features=10, bias=True)\n",
      "        (modules_to_save): ModuleDict(\n",
      "          (default): Linear(in_features=768, out_features=10, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(lora_peft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85f293fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer \n",
    " \n",
    "model_name = \"medicalai/ClinicalBERT\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9cbf929c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 38 years old and male, I am experiencing an itchy nose as my main symptom, accompanied by pain, skin lesions or rashes, shortness of breath, and mouth ulcers or sores. The sharp pain is located in my left shoulder, and the rash is red on my nose.  :  0  :  Allergic sinusitis\n"
     ]
    }
   ],
   "source": [
    "# get samples from the tokenized dataset of test split and check some predictions\n",
    "samples = tokenized_ds[\"test\"]\n",
    "sample = samples[100] # Predicted correct\n",
    "# sample = samples[0] # Predicted correct\n",
    "# sample = samples[50] # Predicted wrong\n",
    "\n",
    "print(sample[\"sentence1\"], \" : \", sample['label']\n",
    "      , \" : \", label_feature.int2str(int(sample['label'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "511d5e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: At 38 years old and male, I am experiencing an itchy nose as my main symptom, accompanied by pain, skin lesions or rashes, shortness of breath, and mouth ulcers or sores. The sharp pain is located in my left shoulder, and the rash is red on my nose.\n",
      "Predicted label: Allergic sinusitis\n"
     ]
    }
   ],
   "source": [
    "# Predict the label\n",
    "import numpy as np\n",
    "inputs = tokenizer(sample[\"sentence1\"], return_tensors=\"pt\")\n",
    "outputs = lora_peft(**inputs)\n",
    "predicted_label_id = np.argmax(outputs.logits[0].detach().cpu().numpy())\n",
    "predicted_label = label_feature.int2str(int(predicted_label_id))\n",
    "\n",
    "print(\"Text:\", sample[\"sentence1\"])\n",
    "print(\"Predicted label:\", predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62196248",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearn_env (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
